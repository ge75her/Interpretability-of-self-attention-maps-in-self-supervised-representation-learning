{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhfJWjt9Abkm"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1658349870216,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "NGoqRIVnPaWS"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "PY2 = sys.version_info[0] == 2\n",
    "\n",
    "if PY2:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f)\n",
    "else:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f, encoding=encoding)\n",
    "\n",
    "def _load_data(url, filename):\n",
    "    \"\"\"Load data from `url` and store the result in `filename`.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading MNIST dataset\")\n",
    "        urlretrieve(url, filename)\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return pickle_load(f, encoding='latin-1')\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filename, url=None):\n",
    "    \"\"\"Get data with labels, split into training, validation and test set.\"\"\"\n",
    "    data = _load_data(url,filename)\n",
    "    X_train, y_train = data[0]\n",
    "    X_valid, y_valid = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "\n",
    "\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_valid=X_valid,\n",
    "        y_valid=y_valid,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1932,
     "status": "ok",
     "timestamp": 1658349894836,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "uKu3eWbePfdH",
    "outputId": "df590ea8-5c06-4021-8243-f0f3ab0bb56e"
   },
   "outputs": [],
   "source": [
    "ORG_SHP = [28,28]\n",
    "OUT_SHP = [100,100]\n",
    "NUM_DISTORTIONS = 6\n",
    "dist_size = (9,9)  \n",
    "NUM_DISTORTIONS_DB = 100000\n",
    "\n",
    "mnist_data = load_data('/content/cluttered_mnist/mnist.pkl.gz')\n",
    "np.random.seed(1234)\n",
    "''' mnist dataset mnist.pkl.gz\n",
    "contains: X_train (50000),X_vaild (10000),X_test (10000), each img of size 784\n",
    "input dim:784\n",
    "output_dim:(10)\n",
    "'''\n",
    "### create list with distortions\n",
    "all_digits = np.concatenate([mnist_data['X_train'], mnist_data['X_valid']], axis=0)\n",
    "all_digits = all_digits.reshape([-1] + ORG_SHP) #(600000,28,28)\n",
    "num_digits = all_digits.shape[0] \n",
    "\n",
    "distortions = []\n",
    "for i in range(NUM_DISTORTIONS_DB):\n",
    "    rand_digit = np.random.randint(num_digits)\n",
    "    rand_x = np.random.randint(ORG_SHP[1]-dist_size[1])\n",
    "    rand_y = np.random.randint(ORG_SHP[0]-dist_size[0])\n",
    "\n",
    "    digit = all_digits[rand_digit]\n",
    "    distortion = digit[rand_y:rand_y + dist_size[0],\n",
    "                       rand_x:rand_x + dist_size[1]]\n",
    "    assert distortion.shape == dist_size\n",
    "    distortions += [distortion]\n",
    "print(\"Created distortions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1658349898193,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "utIjmgzgPs8K"
   },
   "outputs": [],
   "source": [
    "def create_sample1(x, output_shp, num_distortions=NUM_DISTORTIONS):\n",
    "    a, b= x.shape\n",
    "    x_offset = (output_shp[1]-a)//2\n",
    "    y_offset = (output_shp[1]-a)//2\n",
    "\n",
    "    x_offset += np.random.choice(range(int(-3*x_offset/2), int(3*x_offset/2)))\n",
    "    y_offset += np.random.choice(range(int(-3*y_offset/2), int(3*y_offset/2)))\n",
    "\n",
    "\n",
    "    angle = np.random.choice(range(int(-b*0.5), int(b*0.5)))\n",
    "\n",
    "    output = np.zeros(output_shp)\n",
    "    \n",
    "    x_start = 0*b+x_offset\n",
    "\n",
    "    x_end = x_start + b\n",
    "    y_start = y_offset + np.floor(0*angle)\n",
    "    y_end = y_start + a\n",
    "    if y_end > (output_shp[1]-1):\n",
    "        m = output_shp[1] - y_end\n",
    "        y_end += m\n",
    "        y_start += m\n",
    "    if y_start < 0:\n",
    "        m = y_start\n",
    "        y_end -= m\n",
    "        y_start -= m\n",
    "    y_start,y_end=int(y_start),int(y_end)\n",
    "    \n",
    "    output[y_start:y_end, x_start:x_end] = x\n",
    "\n",
    "    if num_distortions > 0:\n",
    "            output = add_distortions(output, num_distortions)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def add_distortions(digits, num_distortions):\n",
    "    canvas = np.zeros_like(digits)\n",
    "    for i in range(num_distortions):\n",
    "        rand_distortion = distortions[np.random.randint(NUM_DISTORTIONS_DB)]\n",
    "        rand_x = np.random.randint(OUT_SHP[1]-dist_size[1])\n",
    "        rand_y = np.random.randint(OUT_SHP[0]-dist_size[0])\n",
    "        canvas[rand_y:rand_y+dist_size[0],\n",
    "               rand_x:rand_x+dist_size[1]] = rand_distortion\n",
    "    canvas += digits\n",
    "\n",
    "    return np.clip(canvas, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3404,
     "status": "ok",
     "timestamp": 1658349918570,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "Cur3qC-vPtwZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "# Data Augmentation\n",
    "class DataAugmentation:\n",
    "    def __init__(self,global_crops_scale=(0.5,1.0),n_local_crops=2,output_size=224):\n",
    "        \n",
    "        self.n_local_crops = n_local_crops\n",
    "        RandomGaussianBlur=lambda p: transforms.RandomApply([transforms.GaussianBlur(kernel_size=3,sigma=(0.1,2))],p=p)\n",
    "        flip_and_rotation=transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=(10)),])\n",
    "        colorjitter=transforms.ColorJitter(brightness=0,contrast=0,saturation=0,hue=0.2)\n",
    "        crop=transforms.CenterCrop(28)\n",
    "        resize=transforms.Resize((output_size,output_size),interpolation=InterpolationMode.BICUBIC)\n",
    "        rotation=transforms.RandomRotation(degrees=(6))\n",
    "        shift=transforms.RandomAffine(degrees=6,translate=(0.2,0.1))\n",
    "        normalize=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.13,),(0.3,)),])\n",
    "        \n",
    "\n",
    "        self.global_1=transforms.Compose([\n",
    "            #shift,\n",
    "            #flip_and_rotation,\n",
    "            transforms.RandomResizedCrop(output_size,scale=global_crops_scale,interpolation=InterpolationMode.BICUBIC),\n",
    "            #colorjitter,\n",
    "            rotation,\n",
    "            RandomGaussianBlur(0.1),\n",
    "            normalize\n",
    "        ])\n",
    "        self.global_2=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(output_size,scale=global_crops_scale,interpolation=InterpolationMode.BICUBIC),\n",
    "            rotation,\n",
    "            #colorjitter,\n",
    "            RandomGaussianBlur(1.0),\n",
    "            #transforms.RandomSolarize(170,p=0.2),\n",
    "            normalize\n",
    "        ])\n",
    "        self.local=transforms.Compose([\n",
    "            crop,\n",
    "            resize,\n",
    "            #colorjitter,\n",
    "            rotation,\n",
    "            RandomGaussianBlur(0.5),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def __call__(self,image):\n",
    "        '''\n",
    "        all_crops:list of torch.Tensor\n",
    "        represent different version of input img\n",
    "        '''\n",
    "        all_crops=[]\n",
    "        image=(np.asarray(image.convert('L')))/255.0\n",
    "        \n",
    "        image1=create_sample1(image, OUT_SHP)\n",
    "        image2=create_sample1(image, OUT_SHP)\n",
    "        image1=(image1*255.0).astype(np.uint8)\n",
    "        image2=(image2*255.0).astype(np.uint8)\n",
    "        image1=Image.fromarray(cv2.cvtColor(image1,cv2.COLOR_GRAY2RGB))\n",
    "        image2=Image.fromarray(cv2.cvtColor(image2,cv2.COLOR_GRAY2RGB))\n",
    "\n",
    "        all_crops.append(self.global_1(image1))\n",
    "        \n",
    "\n",
    "        all_crops.append(self.global_1(image2))\n",
    "    \n",
    "        return all_crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYomvtkaAbkr"
   },
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T21:24:59.915973Z",
     "iopub.status.busy": "2022-06-01T21:24:59.915605Z",
     "iopub.status.idle": "2022-06-01T21:24:59.974667Z",
     "shell.execute_reply": "2022-06-01T21:24:59.973741Z",
     "shell.execute_reply.started": "2022-06-01T21:24:59.915941Z"
    },
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1658349923053,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "VAv2AavlAbkt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostly copy-paste from timm library.\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,dim,num_heads=8,qkv_bias=False,qk_scale=None,attn_drop_ratio=0.,proj_drop_ratio=0.):\n",
    "        super(Attention,self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x,attn\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,dim,num_heads, mlp_ratio=4.,qkv_bias=False,qk_scale=None, drop_ratio=0.,attn_drop_ratio=0., drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "\n",
    "        #  drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x,return_attention=False):\n",
    "        y,attn=self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=8, in_c=3, num_classes=0,\n",
    "                 embed_dim=384, depth=6, num_heads=6, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    "\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_vit_weights)\n",
    "    \n",
    "    \n",
    "    def _init_vit_weights(self,m):\n",
    "        \"\"\"\n",
    "        ViT weight initialization\n",
    "        :param m: module\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "#     def forward_features(self, x):\n",
    "#         # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "#         x = self.patch_embed(x)  # [B, 196, 768]\n",
    "#         # [1, 1, 768] -> [B, 1, 768]\n",
    "#         cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "#         if self.dist_token is None:\n",
    "#             x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n",
    "#         else:\n",
    "#             x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "#         x = self.pos_drop(x + self.pos_embed)\n",
    "#         x = self.blocks(x)\n",
    "#         x = self.norm(x)\n",
    "#         if self.dist_token is None:\n",
    "#             return self.pre_logits(x[:, 0])\n",
    "#         else:\n",
    "#             return x[:, 0], x[:, 1]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.forward_features(x)\n",
    "#         if self.head_dist is not None:\n",
    "#             x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "#             if self.training and not torch.jit.is_scripting():\n",
    "#                 # during inference, return the average of both classifier predictions\n",
    "#                 return x, x_dist\n",
    "#             else:\n",
    "#                 return (x + x_dist) / 2\n",
    "#         else:\n",
    "#             x = self.head(x)\n",
    "#         return x\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "            \n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t75_iQwmAbkv"
   },
   "source": [
    "# new Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.160336Z",
     "iopub.status.busy": "2022-06-01T19:27:35.157832Z",
     "iopub.status.idle": "2022-06-01T19:27:35.179816Z",
     "shell.execute_reply": "2022-06-01T19:27:35.179079Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.160300Z"
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1658349930163,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "qEYefO_RAbkw"
   },
   "outputs": [],
   "source": [
    "class DINOHead(nn.Module):\n",
    "    \"\"\"Network hooked up to the CLS token embedding.\n",
    "    Just a MLP with the last layer being normalized in a particular way.\n",
    "    \n",
    "    Parameters:\n",
    "    in_dim : int\n",
    "        The dimensionality of the token embedding.\n",
    "    out_dim : int\n",
    "        The dimensionality of the final layer (we compute the softmax over).\n",
    "    hidden_dim : int\n",
    "        Dimensionality of the hidden layers.\n",
    "    bottleneck_dim : int\n",
    "        Dimensionality of the second last layer.\n",
    "    n_layers : int\n",
    "        The number of layers.\n",
    "    norm_last_layer : bool\n",
    "        If True, then we freeze the norm of the weight of the last linear layer\n",
    "        to 1.\n",
    "        \n",
    "        \n",
    "    Attributes:\n",
    "    mlp : nn.Sequential\n",
    "        Vanilla multi-layer perceptron.\n",
    "    last_layer : nn.Linear\n",
    "        Reparametrized linear layer with weight normalization. That means\n",
    "        that that it will have `weight_g` and `weight_v` as learnable\n",
    "        parameters instead of a single `weight`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=512, bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.apply(self._init_weights)\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor\n",
    "            Of shape `(n_samples, in_dim)`.\n",
    "        \n",
    "        return: torch.Tensor\n",
    "            Of shape `(n_samples, out_dim)`.\n",
    "        \"\"\"\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdCivSGMAbkw"
   },
   "source": [
    "# Multicropwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.188015Z",
     "iopub.status.busy": "2022-06-01T19:27:35.185582Z",
     "iopub.status.idle": "2022-06-01T19:27:35.199837Z",
     "shell.execute_reply": "2022-06-01T19:27:35.198951Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.187978Z"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1658349933451,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "h8M8Hs8pAbkx"
   },
   "outputs": [],
   "source": [
    "class MultiCropWrapper(nn.Module):\n",
    "    \"\"\"Convenience class for forward pass of multiple crops.\n",
    "\n",
    "    Parameters:\n",
    "    backbone : vision transformer\n",
    "        Instantiated Vision Transformer. Note that we will take the `head` attribute and replace it with `nn.Identity`.\n",
    "    head : DINOHead\n",
    "        New head that is going to be put on top of the `backbone`.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, head):\n",
    "        super(MultiCropWrapper, self).__init__()\n",
    "        # disable layers dedicated to ImageNet labels classification\n",
    "        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The different crops are concatenated along the batch dimension. The resulting tensor is then chunked back to per crop tensors.\n",
    "        return: list of crops len=n_crops, each of shape (batch,out_dim)\n",
    "        '''\n",
    "        # convert to list\n",
    "        if not isinstance(x, list):\n",
    "            #print('multicrop',x.shape)\n",
    "            x = [x]\n",
    "        n_crops=len(x)\n",
    "        concatenated=torch.cat(x,dim=0)\n",
    "        cls_embedding=self.backbone(concatenated)\n",
    "        logits=self.head(cls_embedding)\n",
    "        chunks=logits.chunk(n_crops)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z-WeOIWAbky"
   },
   "source": [
    "# simCLR loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.206562Z",
     "iopub.status.busy": "2022-06-01T19:27:35.204048Z",
     "iopub.status.idle": "2022-06-01T19:27:35.224697Z",
     "shell.execute_reply": "2022-06-01T19:27:35.223981Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.206525Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1658349935698,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "I_QXtsplAbkz"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, tau=0.1,out_dim=1024,center_momentum=0.995):\n",
    "        super().__init__()\n",
    "        self.tau=tau\n",
    "        self.center_momentum=center_momentum\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "\n",
    "    def forward(self, student_output_ori, teacher_output_ori):\n",
    "        \"\"\"\n",
    "        simCLRLoss of the teacher and student branch.\n",
    "        student_output_ori: list of len=n_crops, each of shape (batch,out_dim)\n",
    "        \"\"\"\n",
    "        teacher_output=torch.cat(teacher_output_ori,dim=1)\n",
    "        student_output=torch.cat(student_output_ori,dim=1)\n",
    "        n_examples,_=student_output.size()\n",
    "        teacher=F.normalize(teacher_output,dim=-1)\n",
    "        student=F.normalize(student_output,dim=-1)\n",
    "        scores=torch.mm(teacher,student.t()).div_(self.tau)\n",
    "        target=torch.arange(n_examples,dtype=torch.long).to(scores.device)\n",
    "        loss=F.cross_entropy(scores,target)\n",
    "        self.update_center(teacher_output_ori)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"Update center used for teacher output.\n",
    "        Compute the exponential moving average.\n",
    "        Parameters\n",
    "        ----------\n",
    "        teacher_output : tuple\n",
    "            Tuple of tensors of shape `(n_samples, out_dim)` where each\n",
    "            tensor represents a different crop.\n",
    "        \"\"\"\n",
    "        batch_center = torch.cat(teacher_output).mean(\n",
    "            dim=0, keepdim=True\n",
    "        )  # (1, out_dim)\n",
    "        self.center = self.center * self.center_momentum + batch_center * (\n",
    "            1 - self.center_momentum\n",
    "        )\n",
    "\n",
    "    \n",
    "def clip_gradients(model, clip=2.0):\n",
    "    \"\"\"Rescale norm of computed gradients. Used to avoid gradient exponential\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Module.\n",
    "    clip : float\n",
    "        Maximum norm.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            clip_coef = clip / (param_norm + 1e-6)\n",
    "            if clip_coef < 1:\n",
    "                p.grad.data.mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.231228Z",
     "iopub.status.busy": "2022-06-01T19:27:35.228896Z",
     "iopub.status.idle": "2022-06-01T19:27:35.240060Z",
     "shell.execute_reply": "2022-06-01T19:27:35.239334Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.231191Z"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1658349939397,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "uIT57jeuAbk0"
   },
   "outputs": [],
   "source": [
    "def get_params_groups(model):\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpYiEt7HQi5D"
   },
   "source": [
    "# MetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.246942Z",
     "iopub.status.busy": "2022-06-01T19:27:35.244425Z",
     "iopub.status.idle": "2022-06-01T19:27:35.288296Z",
     "shell.execute_reply": "2022-06-01T19:27:35.287410Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.246889Z"
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1658349941961,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "v2zOFUJ7Abk1"
   },
   "outputs": [],
   "source": [
    "'''MetricLogger is used to record the result value: loss, learning rate, weight decay and time (same as Tensorboard)'''\n",
    "from collections import defaultdict,deque\n",
    "import datetime\n",
    "import time\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.6f} ({global_avg:.6f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.6f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.6f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.6f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JydCT-NJQi5E"
   },
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.291142Z",
     "iopub.status.busy": "2022-06-01T19:27:35.289468Z",
     "iopub.status.idle": "2022-06-01T19:27:35.304847Z",
     "shell.execute_reply": "2022-06-01T19:27:35.303878Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.291105Z"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1658349946682,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "kGSMHx1rAbk2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "\n",
    "def get_params_groups(model):\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "    \n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iBNUPmbAbk2"
   },
   "source": [
    "# training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.307082Z",
     "iopub.status.busy": "2022-06-01T19:27:35.306371Z",
     "iopub.status.idle": "2022-06-01T19:27:35.317469Z",
     "shell.execute_reply": "2022-06-01T19:27:35.316304Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.307045Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1658349949513,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "JDIILn1aAbk3"
   },
   "outputs": [],
   "source": [
    "def cancel_gradients_last_layer(epoch, model, freeze_last_layer):\n",
    "    if epoch >= freeze_last_layer:\n",
    "        return\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"last_layer\" in n:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:47:08.503717Z",
     "iopub.status.busy": "2022-06-01T19:47:08.503359Z",
     "iopub.status.idle": "2022-06-01T19:47:08.518605Z",
     "shell.execute_reply": "2022-06-01T19:47:08.517802Z",
     "shell.execute_reply.started": "2022-06-01T19:47:08.503686Z"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1658350319196,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "_afriAkWAbk3"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(student,teacher,dino_loss,data_loader,optimizer,lr_schedule,wd_schedule,momentum_schedule,epoch,output_dir\n",
    "                    ,total_epochs,clip_grad,freeze_last_layer):\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Epoch: [{}/{}]'.format(epoch, total_epochs)\n",
    "    for it, (images, _) in enumerate(metric_logger.log_every(data_loader, 3000, header)):\n",
    "        # update weight decay and learning rate according to their schedule\n",
    "        it = len(data_loader) * epoch + it  # global training iteration\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group[\"lr\"] = lr_schedule[it]\n",
    "            if i == 0:  # only the first group is regularized\n",
    "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
    "\n",
    "        # move images to gpu\n",
    "        images = [im.cuda() for im in images]\n",
    "        teacher_output=teacher(images[:4]) #1st image\n",
    "        student_output=student(images[4:]) #2nd image\n",
    "        loss=dino_loss(student_output,teacher_output)\n",
    "        #print('loss:{:.4f}, stopping training'.format(loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        param_norms=clip_gradients(student,clip=clip_grad)\n",
    "        cancel_gradients_last_layer(epoch,student,freeze_last_layer)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #EMA update teacher\n",
    "        with torch.no_grad():\n",
    "            m = momentum_schedule[it]  # momentum parameter\n",
    "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
    "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(wd=optimizer.param_groups[0][\"weight_decay\"])\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1658350322606,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "4azdn0DQixp6"
   },
   "outputs": [],
   "source": [
    "def restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Re-start from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(ckp_path):\n",
    "        return\n",
    "    print(\"Found checkpoint at {}\".format(ckp_path))\n",
    "\n",
    "    # open checkpoint file\n",
    "    checkpoint = torch.load(ckp_path, map_location=\"cpu\")\n",
    "\n",
    "    # key is what to look for in the checkpoint file\n",
    "    # value is the object to load\n",
    "    # example: {'state_dict': model}\n",
    "    for key, value in kwargs.items():\n",
    "        if key in checkpoint and value is not None:\n",
    "            try:\n",
    "                msg = value.load_state_dict(checkpoint[key], strict=False)\n",
    "                print(\"=> loaded '{}' from checkpoint '{}' with msg {}\".format(key, ckp_path, msg))\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    msg = value.load_state_dict(checkpoint[key])\n",
    "                    print(\"=> loaded '{}' from checkpoint: '{}'\".format(key, ckp_path))\n",
    "                except ValueError:\n",
    "                    print(\"=> failed to load '{}' from checkpoint: '{}'\".format(key, ckp_path))\n",
    "        else:\n",
    "            print(\"=> key '{}' not found in checkpoint: '{}'\".format(key, ckp_path))\n",
    "\n",
    "    # re load variable important for the run\n",
    "    if run_variables is not None:\n",
    "        for var_name in run_variables:\n",
    "            if var_name in checkpoint:\n",
    "                run_variables[var_name] = checkpoint[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:48:58.745289Z",
     "iopub.status.busy": "2022-06-01T19:48:58.744849Z",
     "iopub.status.idle": "2022-06-01T19:48:58.903496Z",
     "shell.execute_reply": "2022-06-01T19:48:58.902762Z",
     "shell.execute_reply.started": "2022-06-01T19:48:58.745257Z"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1658350327399,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "vwsOTHmbAbk4"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DistributedSampler,DataLoader\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "def train_dino(data_path,batch_size,lr,weight_decay,weight_decay_end,min_lr,out_dim,tau,total_epochs,warmup_epochs,momentum_teacher,output_dir,saveckp_freq,\n",
    "            clip_grad,freeze_last_layer):\n",
    "\n",
    "    transform = DataAugmentation()\n",
    "    dataset = ImageFolder(data_path, transform=transform)\n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,drop_last=True,)\n",
    "    print(f\"Data loaded: there are {len(dataset)} images.\")\n",
    "\n",
    "    #student =VisionTransformer(patch_size=8,drop_path_ratio=0.1,)  # stochastic depth\n",
    "    #teacher = VisionTransformer(patch_size=8)\n",
    "    student=torch.hub.load('facebookresearch/dino:main', 'dino_vits8',drop_path_rate=0.1,pretrained=True) #use pretrained model to speed up the training process\n",
    "    teacher= torch.hub.load('facebookresearch/dino:main', 'dino_vits8',pretrained=True)\n",
    "    embed_dim = student.embed_dim\n",
    "\n",
    "    student = MultiCropWrapper(student, DINOHead(embed_dim,out_dim=out_dim,use_bn=False,norm_last_layer=True,))\n",
    "    teacher = MultiCropWrapper(teacher,DINOHead(embed_dim, out_dim=out_dim, use_bn=False),)\n",
    "    # move networks to gpu\n",
    "    student=student.cuda()\n",
    "    teacher=teacher.cuda()\n",
    "    params_groups = get_params_groups(student)\n",
    "    optimizer = torch.optim.AdamW(params_groups)\n",
    "    # teacher and student start with the same weights\n",
    "    teacher.load_state_dict(student.state_dict())\n",
    "    dino_loss = Loss(tau=tau,out_dim=out_dim).cuda()\n",
    "    #there is no backpropagation through the teacher, so no need for gradients\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"Student and Teacher are built: they are both vit network.\")\n",
    "\n",
    "\n",
    "    lr_schedule = cosine_scheduler(lr * (batch_size* get_world_size()) / 64.,min_lr, total_epochs, len(data_loader),warmup_epochs=warmup_epochs,)\n",
    "    wd_schedule = cosine_scheduler(weight_decay,weight_decay_end,total_epochs, len(data_loader),)\n",
    "    # momentum parameter is increased to 1. during training with a cosine schedule\n",
    "    momentum_schedule = cosine_scheduler(momentum_teacher, 1,total_epochs, len(data_loader))\n",
    "    print(f\"Loss, optimizer and schedulers ready.\")\n",
    "\n",
    "    #start to training\n",
    "    to_restore = {\"epoch\": 0}\n",
    "    restart_from_checkpoint(\n",
    "        os.path.join(output_dir, \"checkpoint10.pth\"),\n",
    "        run_variables=to_restore,\n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        optimizer=optimizer,\n",
    "        dino_loss=dino_loss,\n",
    "    )\n",
    "    \n",
    "    start_epoch = to_restore[\"epoch\"]\n",
    "    start_time = time.time()\n",
    "    print(\"Starting DINO training !\")\n",
    "    \n",
    "    for epoch in range(start_epoch,total_epochs):\n",
    "        train_stats=train_one_epoch(student,teacher,dino_loss,data_loader,optimizer,lr_schedule,wd_schedule,momentum_schedule,epoch,output_dir,\n",
    "                                    total_epochs,clip_grad,freeze_last_layer)\n",
    "        \n",
    "        save_dict = {'student': student.state_dict(),'teacher': teacher.state_dict(),'optimizer': optimizer.state_dict(),'epoch': epoch + 1,'loss': dino_loss.state_dict()}\n",
    "        if epoch%saveckp_freq==0:\n",
    "            save_on_master(save_dict,os.path.join(output_dir,'checkpoint{}.pth'.format(epoch)))\n",
    "        \n",
    "        log_stats={**{f'train_{k}': v for k, v in train_stats.items()},'epoch': epoch}\n",
    "        with (Path(output_dir)/'log.txt').open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats)+'\\n')\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7f3447095ec84e0880ef076128284f03",
      "5d4fdbd8f2ab4209b6c9940373c61925",
      "b4072c9d02c04ee69364802efae63d2e",
      "cf4470f9eb044224a748be924964be58",
      "f35ba3a559244cdcbf91597962db5903",
      "744a18303ff64e6da2432a8fa47dfd7d",
      "98f17a02ff5244ebac79c85de9d8269b",
      "8ee77f3b98834342824312a0d1cea0f2",
      "44597624b67141268daf4cd1a99e8a58",
      "ab4eaf27c8514579b44aa6fa8426aba0",
      "7723f7eb2ff14b37b7dbfc37e32b8a08"
     ]
    },
    "execution": {
     "iopub.execute_input": "2022-06-01T19:49:03.128724Z",
     "iopub.status.busy": "2022-06-01T19:49:03.128351Z",
     "iopub.status.idle": "2022-06-01T21:09:35.675095Z",
     "shell.execute_reply": "2022-06-01T21:09:35.673622Z",
     "shell.execute_reply.started": "2022-06-01T19:49:03.128695Z"
    },
    "executionInfo": {
     "elapsed": 70372944,
     "status": "error",
     "timestamp": 1658420731141,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "K6eTDhg8Abk5",
    "outputId": "5583f1e0-ec3e-4bab-e743-3f020e7e7310"
   },
   "outputs": [],
   "source": [
    "data_path='/content/cluttered_mnist/mnist/MNIST'\n",
    "batch_size=128\n",
    "lr=1e-4\n",
    "weight_decay=0.04\n",
    "weight_decay_end=0.4\n",
    "min_lr=1e-8\n",
    "\n",
    "total_epochs=40\n",
    "warmup_epochs=3\n",
    "momentum_teacher=0.995\n",
    "out_dim=1024\n",
    "tau=0.1\n",
    "saveckp_freq=3\n",
    "\n",
    "clip_grad=3.0\n",
    "freeze_last_layer=1\n",
    "output_dir='/content/drive/MyDrive/cluttered_mnist_diff/logs'\n",
    "\n",
    "train_dino(data_path,batch_size,lr,weight_decay,weight_decay_end,min_lr,out_dim,tau,total_epochs,warmup_epochs,momentum_teacher,output_dir,saveckp_freq,\n",
    "            clip_grad,freeze_last_layer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "whole_pipline(1)(1)(1).ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f727375eba1284a63b4d9e638236d85d45c674565c6cacd6cfe2f9879757f3bd"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "44597624b67141268daf4cd1a99e8a58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d4fdbd8f2ab4209b6c9940373c61925": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_744a18303ff64e6da2432a8fa47dfd7d",
      "placeholder": "​",
      "style": "IPY_MODEL_98f17a02ff5244ebac79c85de9d8269b",
      "value": "100%"
     }
    },
    "744a18303ff64e6da2432a8fa47dfd7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7723f7eb2ff14b37b7dbfc37e32b8a08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f3447095ec84e0880ef076128284f03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d4fdbd8f2ab4209b6c9940373c61925",
       "IPY_MODEL_b4072c9d02c04ee69364802efae63d2e",
       "IPY_MODEL_cf4470f9eb044224a748be924964be58"
      ],
      "layout": "IPY_MODEL_f35ba3a559244cdcbf91597962db5903"
     }
    },
    "8ee77f3b98834342824312a0d1cea0f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98f17a02ff5244ebac79c85de9d8269b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab4eaf27c8514579b44aa6fa8426aba0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4072c9d02c04ee69364802efae63d2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ee77f3b98834342824312a0d1cea0f2",
      "max": 86728949,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_44597624b67141268daf4cd1a99e8a58",
      "value": 86728949
     }
    },
    "cf4470f9eb044224a748be924964be58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab4eaf27c8514579b44aa6fa8426aba0",
      "placeholder": "​",
      "style": "IPY_MODEL_7723f7eb2ff14b37b7dbfc37e32b8a08",
      "value": " 82.7M/82.7M [00:05&lt;00:00, 22.9MB/s]"
     }
    },
    "f35ba3a559244cdcbf91597962db5903": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
